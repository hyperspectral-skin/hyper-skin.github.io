{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Hyper-Skin 2023 - NeurIPS 2023 Mobile Skin Analysis through Hyperspectral Imaging Reconstruction Project Vision <p>         This project aims to develop an innovative framework for hyperspectral skin analysis by leveraging the Hyper-Skin dataset, which spans from the visible (VIS) to the near-infrared (NIR) spectrum. Our goal is to enable accurate skin spectra reconstruction from RGB images, facilitating the study of skin characteristics like melanin and hemoglobin concentrations directly on consumer devices.     </p> <p>         The Hyper-Skin dataset is composed of 306 hyperspectral cubes from 51 subjects, capturing facial skin from three different angles and two facial poses. Each hyperspectral cube has a dimension of 1024x1024x448, and has been resampled into two separate 31-band datasets, covering the VIS spectrum from 400nm to 700nm and the NIR spectrum from 700nm to 1000nm. This extensive dataset enables the study of both surface-level and deeper tissue properties of the skin, offering valuable insights for applications in cosmetology and personalized skincare.     </p> <p>         By bridging the gap between hyperspectral imaging and accessible technology, we seek to advance research in cosmetology and personalized skincare, fostering new applications that enhance skin well-being.     </p> Contact and Collaboration <p>We are open to collaboration and further research. If you are interested in working with us or have any questions, feel free to contact the team via the contact page.</p>"},{"location":"about/","title":"Team","text":"<p>This initiative, focused on advancing Hyperspectral Imaging (HSI) technology, is led by experts from leading institutions including the University of Toronto, Singapore Institute of Technology and industry partners. Together, we aim to develop cutting-edge algorithms for hyperspectral data reconstruction and analysis, paving the way for innovative applications in healthcare and beyond.</p> DR. NG PAI CHET Lorem Ipsum <p>       Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.     </p> DR. KONSTANTINOS (KOSTAS) N. PLATANIOTIS Lorem Ipsum <p>      Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.     </p> DR. JUWEI LU Lorem Ipsum <p>      Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.     </p> DR. MALCOLM YOKE HEAN LOW Lorem Ipsum <p>      Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.     </p> TEO HAO HAN TRAVIS Lorem Ipsum <p>      Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.       </p>"},{"location":"contact/","title":"Contact Us","text":"Send"},{"location":"hyper-skin-2023/","title":"Hyper-Skin-2023","text":"Hyper-Skin 2023 - NeurIPS 2023 Hyper-Skin 2023 at NeurIPS 2023 <p>Welcome to the Hyper-Skin 2023 project page for NeurIPS 2023. This page provides an overview of our research, dataset, and code related to hyperspectral imaging reconstruction from RGB images. Please use the links below for easier reference of your material of choice.</p> Paper Video Code Data Access Request Poster Slides Introduction <p>Introducing Hyper-Skin, a uniquely designed hyperspectral dataset aiming to revolutionize hyperspectral skin analysis on consumer devices. With spatial and spectral resolution of 1024 \u00d7 1024 \u00d7 448, Hyper-Skin offers an extensive collection of hyperspectral cubes, providing over a million spectra per image.</p> <p>A notable feature of Hyper-Skin is the inclusion of synthesized RGB images generated from 28 real camera response functions, enabling versatile experimental setups. What sets Hyper-Skin apart is its comprehensive spectral coverage, including both the visible (VIS) spectrum spanning from 400nm to 700nm and near-infrared (NIR) spectrum from 700nm to 1000nm, facilitating a holistic understanding of various aspects of human facial skin.</p> <p>This dataset enables new possibilities for consumer applications to see beyond the visual appearance of their selfies and gain valuable insights into their skin's physiological characteristics, such as melanin and hemoglobin concentrations.</p> <p>With the Hyper-Skin dataset, we aim to facilitate ongoing research in facial skin-spectra reconstruction on consumer devices, bringing affordable hyperspectral skin analysis directly to the consumer's fingertips.</p> Paper: Data Collection and Reconstruction Process <p>Our paper, presented at NeurIPS 2023, describes the comprehensive data collection process and the innovative reconstruction technique developed for this project. The main contributions of the paper are:</p> <ul> <li>Data Collection: A detailed description of the data acquisition methods used to collect high-quality hyperspectral images, including equipment setup, calibration, and preprocessing steps.</li> <li>Reconstruction Process: An in-depth explanation of the reconstruction methodology that translates RGB images into hyperspectral images. This section covers the underlying algorithms, optimization techniques, and model architecture used for this purpose.</li> </ul> <p>Access the Paper</p> Data: Hyper-skin-2023 Dataset <p>The Hyper-Skin dataset consists of 306 hyperspectral data samples collected from 51 participants. Each participant contributed 6 images, covering 2 types of facial expressions and 3 different face poses. This diverse image collection ensures a broad representation of poses and facial expressions commonly encountered in selfies. The RAW hyperspectral data underwent radiometric calibration and were resampled into two distinct 31-band datasets. One dataset covers the visible spectrum, ranging from 400nm to 700nm, while the other dataset covers the near-infrared spectrum, ranging from 700nm to 1000nm. Additionally, synthetic RGB and Multispectral (MSI) data were generated, including RGB images and an infrared image at 960nm. The Hyper-Skin dataset comprises two types of data: (RGB, VIS) and (MSI, NIR), offering different needs in skin analysis and facilitates comprehensive investigations of various skin features.</p> <p>Pair of (RGB, VIS) Data</p> <p>The visible spectrum data in the Hyper-Skin dataset allows for the analysis of surface-level skin characteristics...</p> <p>Example of (RGB, VIS) Pair:</p> <p>Images above are used with explicit consent from the participants and cannot be used elsewhere without prior consent.</p> <p>Pair of (MSI, NIR) Data</p> <p>The near-infrared spectrum data included in the Hyper-Skin dataset facilitates the study of deeper tissue properties...</p> <p>Example of (MSI, NIR) Pair:</p> <p>Images above are used with explicit consent from the participants and cannot be used elsewhere without prior consent.</p> Dataset Attributes <p>The Hyper-Skin dataset offers a collection of 306 hyperspectral cubes, meticulously gathered from 51 unique subjects. Each subject\u2019s facial skin is captured across three distinct angles (front, left, and right), and features two facial expressions (neutral and smile), ensuring diverse data to support robust facial skin analysis.</p> <p>The hyperspectral images possess an impressive spatial resolution of 1024 \u00d7 1024 and capture 448 spectral bands. To facilitate targeted analysis, the data has been resampled into two 31-band datasets, one spanning the visible spectrum (VIS) from 400nm to 700nm and the other covering the near-infrared spectrum (NIR) from 700nm to 1000nm, both at 10nm steps.</p> Description (RGB, VIS) (MSI, NIR) Input RGB MSI (RGB + Infrared at 960nm) Output VIS (400nm - 700nm) NIR (700nm - 1000nm) Skin physiological features Surface-level characteristics (e.g., pigmentation, melanin map) Deeper tissue properties (e.g., collagen content, hemoglobin map) <p>Below is an example of the VIS and NIR spectrum graphs being generated from hyperspectral data:</p> Dataset Collection <p>Compliant with University research ethics protocols, the dataset collection process was carefully designed to ensure accuracy and reliability. The study was conducted under the approved protocol RIS-42284, and participants provided informed consent before data collection.</p> <p>Data acquisition was performed using the Specim FX10 camera, a pushbroom camera known for its precision in hyperspectral imaging. The camera was mounted on a customized scanner to ensure precise scanning, and the subject's face was placed 40 cm from the camera. To enhance image quality, halogen lights were positioned on either side of the subject, providing optimal illumination.</p> <p>Each hyperspectral image was captured with a frame rate of 45Hz for one line, taking approximately 22.7 seconds to scan all 1024 lines, covering the full range from 400nm to 1000nm. This meticulous approach ensured the high quality of the 448 spectral bands.</p> <p>The dataset's demographic spans 51 participants aged between their 10s and 50s, with a gender distribution favoring males over females. The participants represent diverse ethnic backgrounds, including Asian, European, and Latino.</p> Code: Hyperspectral Image Reconstruction <p>We provide the complete codebase for reconstructing hyperspectral images from RGB images. The code includes scripts for data preprocessing, model training, and evaluation, as well as implementation of the algorithms described in our paper.</p> Objective <p>The primary objective of the provided code is to reconstruct hyperspectral images from standard RGB images efficiently and accurately. It includes:</p> <ul> <li>Data preprocessing pipelines.</li> <li>Model architectures and training scripts.</li> <li>Evaluation metrics and visualization tools.</li> </ul> <p>Access the Code on GitHub</p>"},{"location":"hyper-skin-2023/#citation","title":"Citation","text":"<p>If you used the dataset for your publication, kindly acknowledge and cite the following work:           <pre><code>  @inproceedings{ng2023hyperskin,\n    title={Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial Skin-Spectra from {RGB} Images},\n    author={Pai Chet Ng and Zhixiang Chi and Yannick Verdie and Juwei Lu and Konstantinos N Plataniotis},\n    booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n    year={2023},\n    url={https://openreview.net/forum?id=doV2nhGm1l}\n    }\n</code></pre>"},{"location":"icassp-hs2024/","title":"ICASSP-HS2024","text":"ICASSP-HS2024 Grand Challenge ICASSP-HS2024 Grand Challenge <p>Welcome to the ICASSP-HS2024 Grand Challenge page. This page contains information about the competition, including details on the dataset, task, results, and code submissions. Please use the links below for easier reference of your material of choice.</p> Paper Background About ICASSP <p>                The IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) is one of the world\u2019s most renowned conferences in the field of signal processing. Organized annually by the IEEE Signal Processing Society, ICASSP brings together researchers, practitioners, and academics from around the globe to present their latest research findings and innovations.              </p> <p>                The conference covers a broad range of topics, including audio and speech processing, image and video processing, machine learning, bioinformatics, and many other areas related to signal processing. ICASSP serves as a platform for experts to exchange ideas, foster collaboration, and contribute to the advancement of the signal processing field.             </p> GC-5: Hyperspectral Skin Vision Challenge <p>                As part of ICASSP 2024, the GC-5: Hyperspectral Skin Vision Challenge was introduced, focusing on the reconstruction of skin spectral reflectance in both the visible (VIS) and near-infrared (NIR) spectral ranges. This grand challenge is designed to push the boundaries of what can be achieved with hyperspectral imaging, specifically targeting applications in the cosmetic and beauty industries.             </p> <p>                The challenge involves reconstructing hyperspectral information from RGB images captured by everyday cameras, alongside NIR data at 960nm. The goal is to make rich hyperspectral information accessible on consumer devices, paving the way for personalized beauty and skincare solutions. This competition is a significant step towards democratizing skin analysis technology, making it available directly on consumer devices like smartphones.             </p> <p>                The GC-5 challenge is organized by leading experts in the field, including Konstantinos Plataniotis, Juwei Lu, Pai Chet Ng, and Zhixiang Chi. Their efforts aim to bring together researchers, machine learning experts, and cosmetic professionals to advance the field of beauty technology through this competition.             </p> <p>                For more detailed information about the challenge, you can visit the official challenge website.             </p> Dataset <p>The dataset used for the challenge is a carefully selected subset of the comprehensive data collected for the NeurIPS 2023 project. It includes hyperspectral images alongside corresponding RGB and NIR (960nm) images, specifically chosen to support the objectives of this competition.</p> Task Description <p>The main task of the ICASSP-HS2024 Grand Challenge was to reconstruct hyperspectral images from the given RGB+NIR (960nm) data. Participants were tasked with developing algorithms capable of generating high-quality hyperspectral reconstructions, which were then evaluated against a ground truth dataset.</p> Competition Results Leaderboard <p>Below is the leaderboard showcasing the top 5 teams in the ICASSP-HS2024 Grand Challenge:</p> Rank Team Name Score Details 1 Team Alpha 95.2 More Info 2 Team Beta 93.7 More Info 3 Team Gamma 92.1 More Info 4 Team Delta 90.8 More Info 5 Team Epsilon 89.5 More Info Code Submissions <p>All code submissions for the ICASSP-HS2024 Grand Challenge are publicly available on GitHub. This includes the helper functions we provided and the repositories from the top 5 winning teams.</p> Helper Functions <p>We have made several key helper functions available to the public to assist with hyperspectral image reconstruction. These utilities can be accessed in our dedicated repository:</p> <p>Visit the helper functions repository here: ICASSP-HS2024 Helper Functions</p> Winning Teams' Code <p>The following are the repositories of the top 5 winning teams from the ICASSP-HS2024 Grand Challenge. These repositories contain the code they submitted for the competition:</p> <ul> <li>Team Alpha's Repository</li> <li>Team Beta's Repository</li> <li>Team Gamma's Repository</li> <li>Team Delta's Repository</li> <li>Team Epsilon's Repository</li> </ul>"},{"location":"icassp-hs2024/#citation","title":"Citation","text":"<p>If you used the dataset for your publication, kindly acknowledge and cite the following work:              <pre><code>  @INPROCEEDINGS{10626113,\n\n  author={Ng, Pai Chet and Chi, Zhixiang and Low, Malcolm and Lu, Juwei and Plataniotis, Konstantinos N and Boulgouris, Nikolaos and Bourlai, Thirimachos and Ro, Yong Man},\n  booktitle={2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, \n  title={Hyperspectral Skin Vision Challenge: Can Your Camera See Beyond Your Skin?}, \n  year={2024},\n  volume={},\n  number={},\n  pages={59-60},\n  keywords={Reflectivity;Signal processing;Reconstruction algorithms;Propulsion;Cameras;Skin;Product development;hyperspectral skin vision;skin spectral reconstruction},\n  doi={10.1109/ICASSPW62465.2024.10626113}}\n</code></pre>"}]}